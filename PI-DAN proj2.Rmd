---
title: "proj2"
author: "Xuanfu Lu"
partner: "Honghao Huang"
date: "4/16/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'report_images/')
```

```{r loading packages}
# graphing
library(ggplot2)
library(gridExtra)
library(plotly)
library(corrgram)
library(corrplot)

# maybe helpful in the PCA analysis
library(ggfortify)  
library(FactoMineR)
library(factoextra)

# CV & modeling
library(MASS)
library(caret)
library('e1071')

# ROC
library(ROCR)
library(gplots)

# AIC
library(mvtnorm)
```

```{r read the image_data}
# read image_data (this step can take quite some time)
image1 <- read.csv(file = "image_data/image1.txt", sep ="", header = FALSE)
image2 <- read.csv(file = "image_data/image2.txt", sep ="", header = FALSE)
image3 <- read.csv(file = "image_data/image3.txt", sep ="", header = FALSE)

# change the column names for better understanding
feature_name <- c("y", "x", "expert", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")
colnames(image1) <- feature_name
colnames(image2) <- feature_name
colnames(image3) <- feature_name

# aggregate the data
image0 <- rbind(image1, image2, image3)
image0_labeled <- image0[image0$expert!=0,]

# Ham: make the same for image 1,2,3 as well
image1_labeled <- image1[image1$expert != 0,]
image2_labeled <- image2[image2$expert != 0,]
image3_labeled <- image3[image3$expert != 0,]
```

```{r auxiliary functions}
source("auxiliary_func.R")
```

# Part I: Data Collection and Exploration
## a) no code
This part involves no code.
## b)
### summarize the data
```{r class percentage}
# percentage of pixels for the different classes (total image)
class_percentage <- rbind(percent(table(num2class(image1$expert))/nrow(image1)),
                          percent(table(num2class(image2$expert))/nrow(image2)),
                          percent(table(num2class(image3$expert))/nrow(image3)),
                          percent(table(num2class(image0$expert))/nrow(image0)))
rownames(class_percentage) <- c("image1", "image2", "image3", "total")
colnames(class_percentage) <- c("clear", "cloudy", "unlabeled")
class_percentage

# percentage of pixels for the different classes (labeled only)
class_percentage <- rbind(percent(table(num2class(image1_labeled$expert))/nrow(image1_labeled)),
                          percent(table(num2class(image2_labeled$expert))/nrow(image2_labeled)),
                          percent(table(num2class(image3_labeled$expert))/nrow(image3_labeled)),
                          percent(table(num2class(image0_labeled$expert))/nrow(image0_labeled)))
rownames(class_percentage) <- c("image1_labeled", "image2_labeled", "image3_labeled", "total_labeled")
colnames(class_percentage) <- c("clear", "cloudy")
class_percentage
```

```{r general summary}
# genearl summary of the aggregated image data
summary(image0)

# Ham: add some summary just in case
summary(image1)
summary(image2)
summary(image3)
```
SUMMARY: for entire image data, the range of x, y coordinats is [65,369] and [2,383], respectively.
SUMMARY: DF,CF,BF,AF,AN have similar range but in decreasing order.
SUMMARY: in the labeled data, number of clear pixel is bigger than number of cloudy pixel.
### plot well-labeled maps
```{r graphs}
# graphing (this step can take quite some time)
# note: I flipped the y-axis to make graphs consistent with graphs in the paper
i1 <- ggplot(data = image1, aes(x = x, y = y)) +
  geom_point(colour = num2col(image1$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 1") + scale_y_reverse()

i2 <- ggplot(data = image2, aes(x = x, y = y)) +
  geom_point(colour = num2col(image2$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 2") + scale_y_reverse()

i3 <- ggplot(data = image3, aes(x = x, y = y)) +
  geom_point(colour = num2col(image3$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 3") + scale_y_reverse()

grid.arrange(i1,i2,i3, nrow=1)
```
OBSERVATION: Clearly, cloudness shows some grouping, which means cloudy area is more likely to have its neighbors being cloudy, and vice versa. Also, the shape and size of the cloudy areas are irregular, although most cloudy areas have an ellipsoid outline.
CONCLUSION: i.i.d. assumption is not justified for this data set.
##c) EDA
### pairwise relationships
```{r overall correlogram}
# cannot run corrgram() function, takes too long
# try corrplot() function
corrplot.mixed(cor(image0), lower="number", upper="color", order="hclust", tl.col = "black")
```
OBSERVATION: NDAI is positively correlated with SD, expert label (not very strong correlation though); negatively correlated   with all the rest features.
OBSERVATION: DF,AF,AN,CF,BF are positively correlated with each other (this is very intuitive). [validated that these measurements make sense, it's unlikely the case that one of the camera is malfuncitoning]
OBSERVATION: y coordinate is more related to radiance reading than x coordinate.
```{r PCA among radiance}
# create a data frame containing all 5 radiance readings in the entire data (including unlabeled)
radiance <- image0[,7:11]
# visualize PCA (onto first 2 PC) using standardized data
radiance_std <- scale(radiance, center=TRUE, scale=TRUE)
#Ham: tweak a little to incoporate quantitative information of pca
radiance_pca <- PCA(radiance_std, graph=FALSE)
#Ham: eigenvalues and variance explained 
radiance_pca$eig
fviz_pca_var(radiance_pca, col.var = "red")
# screeplot
eigenvalues <- eigen(t(radiance_std) %*% radiance_std)$values
plot(cumsum(eigenvalues) / sum(eigenvalues), main = "Scree Plot", ylab = "Eigenvalues", xlab = "Component Number")
```
OBSERVATION: all radiance readings are positively correlated (they lie in the same direction).
OBSERVATION: 1st principal component captures 84.6% of variance. first 2 PCs capture above 96% of the variability.
CONCLUSION: studying the radiance readings themself may not have much results; some transformation would be very helpful; the NDAI feature created by Yu Bin and her team is very brilliant in terms of capturing the changes in a scene with changes in the MISR view direction.
### differences between two classes
```{r histogram by class}
# used auxiliary function to keep it looking tidy
hist_ft <- feature_hist_by_class(image0_labeled)
hist_ft[[1]]
hist_ft[[2]]
hist_ft[[3]]
grid.arrange(hist_ft[[4]], hist_ft[[5]], hist_ft[[6]], hist_ft[[7]], hist_ft[[8]])
```
OBSERVATION: NDAI feature shows strong diversification for cloudy and clear areas. Very clearly, clear areas have less NDAI than cloudy areas. This feature could be very helpful in later part of the project.
OBSERVATION: radiance reading: clear areas seem to have a bimodal histogram of radiance reading (all 5 of them) while cloudy areas have single-mode histogram of radiance reading. Cloudy areas have longer left-side tails. Clear areas seem to have slightly higher radiance reading (this is consistent with the paper finding as well).
OBSERVATION: clear areas tend to have smaller SD (shorter right tails, peak around 0) than cloudy areas.
OBSERVATION: clear areas tend to have smaller and less spreading CORR than cloudy areas.
```{r numeric: difference across class}
summary(image0[which(image0$expert == 1),]$NDAI)
summary(image0[which(image0$expert == -1),]$NDAI)

summary(image0[which(image0$expert == 1),]$SD)
summary(image0[which(image0$expert == -1),]$SD)

summary(image0[which(image0$expert == 1),]$CORR)
summary(image0[which(image0$expert == -1),]$CORR)
```

# Part II: Preparation
## a) split data
```{r method1: by image}
# training data is one of the images
# validation data is one of the other two images
# the last image is the testing data
data_tr1 <- image1
data_va1 <- image2
data_te1 <- image3
# Yu bin says this is NOT OK
```

```{r method2: divide & forge}
# images are already ordered by coordiantes
# first divide the images:
K <- 6
set.seed(154668) # the next few lines of code involve random sampling, set seed to be consistent.
images_divided <- list(divide_image(image1, K), divide_image(image2, K), divide_image(image3, K))

# forge the new images using divided pieces:
new_images <- forge_new_image(images_divided, K)
data_tr2 <- new_images[[1]][new_images[[1]]$expert != 0,]
data_va2 <- new_images[[2]][new_images[[2]]$expert != 0,]
data_te2 <- new_images[[3]][new_images[[3]]$expert != 0,]

# the forged image should look like:
ggplot(data = data_te2, aes(x = x, y = y)) +
  geom_point(colour = num2col(data_te2$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "forged new image") + scale_y_reverse()
```

```{r method3: divide & random sample}
# essentially this would be the same as method2
# method2 is stratified random sampling
# method3 is simple random sampling
# they are essentially the same
# this method will not have a graph
set.seed(123456)
# images_divided is already created

# forge the new images using divided pieces:
new_images <- forge_random(images_divided, K)
data_tr3 <- new_images[[1]][new_images[[1]]$expert != 0,]
data_va3 <- new_images[[2]][new_images[[2]]$expert != 0,]
data_te3 <- new_images[[3]][new_images[[3]]$expert != 0,]

ggplot(data = data_te3, aes(x = x, y = y)) +
  geom_point(colour = num2col(data_te3$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "forged new image") + scale_y_reverse()
```
the image is normal, because this is completely randomly sampling.
there exist some missing areas because there exist equal number of overlapping areas.
```{r method4: blur image}
# read the blurred images (they are constructed using a separate R. file)
# constructing superpixels are computational complex, takes too much time, thus done separately
image1_blur <- read.csv(file = "image_data/image1_blur.csv", header = TRUE)
image2_blur <- read.csv(file = "image_data/image2_blur.csv", header = TRUE)
image3_blur <- read.csv(file = "image_data/image3_blur.csv", header = TRUE)
image1_blur <- image1_blur[,2:12]
image2_blur <- image2_blur[,2:12]
image3_blur <- image3_blur[,2:12]
image0_blur <- rbind(image1_blur, image2_blur, image3_blur)

# randomly select pixels from the all constructed super pixels:
temp_storage <- rep(list(NA), 3)
select_index <- 1:nrow(image0_blur)
for (i in 1:3) {
  temp_index <- sample(na.omit(select_index), size = nrow(image0_blur) %/% 3)
  select_index[temp_index] <- NA
  temp_storage[[i]] <- image0_blur[temp_index,]
}
data_tr4 <- temp_storage[[1]]
data_va4 <- temp_storage[[2]]
data_te4 <- temp_storage[[3]]

# plot the blurred image (just to demonstrate)
i1 <- ggplot(data = image1_blur, aes(x = x, y = y)) +
  geom_point(colour = num2col(image1_blur$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 1 (super-pixel)") + scale_y_reverse()
i2 <- ggplot(data = image2_blur, aes(x = x, y = y)) +
  geom_point(colour = num2col(image2_blur$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 2 (super-pixel)") + scale_y_reverse()
i3 <- ggplot(data = image3_blur, aes(x = x, y = y)) +
  geom_point(colour = num2col(image3_blur$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "Image 3 (super-pixel)") + scale_y_reverse()
grid.arrange(i1,i2,i3, nrow=1)
```
method4 could potentially be the most helpful method, but the drawback is also very obvious: reduced number of observations
method4 takes account of dependency of pixels with neighborhood. In later part we will show that x, y coordinates are not our "best" features, and they will not be passed into modeling. Therefore, method4 (blurring / smoothing) is the only way to take dependancy into account for modeling.
## b) trivial classifier
The best splitting method's accuracy using a trivial classifier should be around the true percent of "clear" label in the entire given data. It means that the splitting does a good job in representing the whole data.
The trivial classifier will have high accuracy if the data split is not good in the sense that it splits almost all the terrain into the testing data. It is the only occassian where the trivial classifier obtains high accuracy. However, this splitting is problematic because it does not represent the reality. Because the trivial classifier does not do well according to our way of splitting the data, we can be more certain that our splitting method is not trivial.
Also, for methods with "good" trivial accuracy, we will run additional stability test to select the best splitting method.
```{r method1}
# since method1 only has 6 possible combinations, we are going to exhaust
a1 <- sum(image1_labeled$expert == -1)/nrow(image1_labeled)
a2 <- sum(image2_labeled$expert == -1)/nrow(image2_labeled)
a3 <- sum(image3_labeled$expert == -1)/nrow(image3_labeled)
accuracy_report1 <- rbind(c(a2,a3), c(a3,a2), c(a1,a2), c(a2,a1), c(a1,a3), c(a3,a1))
accuracy_report1 <- rbind(accuracy_report1, c(mean(accuracy_report1[,1]), mean(accuracy_report1[,2])))
accuracy_report1 <- as.data.frame(accuracy_report1)
colnames(accuracy_report1) <- c("val_accuracy","test_accuracy")
rownames(accuracy_report1) <- c(1:6,"average")
accuracy_report1
```
OBSERVATION: trivial splitting result is not very good, the stability is very week (we can tell this without calculating the sd), and the average is away from the true label percentage.
keep in mind that the total observations has 61.08% of pixels labeled as "clear".
```{r method2}
stability_report2 <- data.frame()
# run method 2 by changing seed
for (i in 1:10) {
  set.seed(i + 500) # change seed in each iteration
  # split the data
  new_images <- forge_new_image(images_divided, K)
  data_tr2 <- new_images[[1]][new_images[[1]]$expert != 0,]
  data_va2 <- new_images[[2]][new_images[[2]]$expert != 0,]
  data_te2 <- new_images[[3]][new_images[[3]]$expert != 0,]
  # report accuracy
  a1 <- sum(data_va2$expert == -1)/nrow(data_va2)
  a2 <- sum(data_te2$expert == -1)/nrow(data_te2)
  stability_report2 <- rbind(stability_report2, data.frame(val_accuracy = a1, test_accuracy = a2))
}
# round off the numbers (purely aesthetic purposes)
stability_report2 <- round(stability_report2, digits = 5)
temp_v_mean <- round(mean(stability_report2$val_accuracy), digit = 5)
temp_v_sd <- round(sd(stability_report2$val_accuracy), digit = 5)
temp_t_mean <- round(mean(stability_report2$test_accuracy), digit = 5)
temp_t_sd <- round(sd(stability_report2$test_accuracy),  digit = 5)

stability_report2 <- rbind(stability_report2, 
                           data.frame(val_accuracy = "---------", test_accuracy = "---------"), 
                           data.frame(val_accuracy = temp_v_mean, test_accuracy = temp_t_mean), 
                           data.frame(val_accuracy = temp_v_sd, test_accuracy = temp_t_sd))
rownames(stability_report2) <- c(1:10,"----", "mean", "sd")

stability_report2
```
OBSERVATION: method2 (split & forge)'s accuracy using trivial classifier is closer to the true percent of "clear" labels in the whole data, indicating it's a better way to split the data than the method1 (trivial splitting method). 
OBSERVATION: method2 has different average accuracy on validation and testing, indicating that validation set is slightly different from the testing set (in terms of % clear labels). This is probably associated with the "stratified sampling" idea we incorporated. However, since the difference is within 1 sd; it's also very likely due to chance. Therefore, we do not think this is an evidence against such method.
OBSERVATION: method2's sd siginificantly decreased, after we increase K from 4 to 6. Such low sd (high stability) makes method2 very appealing. But it could also mean that such method is very dependent on how many sections we divide.
```{r method3}
stability_report3 <- data.frame()
# run method 3 by changing seed
for (i in 1:10) {
  set.seed(i + 500) # change seed in each iteration
  # split the data
  new_images <- forge_random(images_divided, K)
  data_tr3 <- new_images[[1]][new_images[[1]]$expert != 0,]
  data_va3 <- new_images[[2]][new_images[[2]]$expert != 0,]
  data_te3 <- new_images[[3]][new_images[[3]]$expert != 0,]
  # report accuracy
  a1 <- sum(data_va3$expert == -1)/nrow(data_va3)
  a2 <- sum(data_te3$expert == -1)/nrow(data_te3)
  stability_report3 <- rbind(stability_report3, data.frame(val_accuracy = a1, test_accuracy = a2))
}

# round off the numbers (purely aesthetic purposes)
stability_report3 <- round(stability_report3, digits = 5)
temp_v_mean <- round(mean(stability_report3$val_accuracy), digit = 5)
temp_v_sd <- round(sd(stability_report3$val_accuracy), digit = 5)
temp_t_mean <- round(mean(stability_report3$test_accuracy), digit = 5)
temp_t_sd <- round(sd(stability_report3$test_accuracy),  digit = 5)

stability_report3 <- rbind(stability_report3, 
                           data.frame(val_accuracy = "---------", test_accuracy = "---------"), 
                           data.frame(val_accuracy = temp_v_mean, test_accuracy = temp_t_mean), 
                           data.frame(val_accuracy = temp_v_sd, test_accuracy = temp_t_sd))
rownames(stability_report3) <- c(1:10,"----", "mean", "sd")

stability_report3
```
OBSERVATION: method3 has higher sd than method2. However, method3 has average accuracy very similar across validation and testing.
OBSERVATION: method3's sd is not changing much, as we increase K from 4 to 6.
```{r method4}
stability_report4 <- data.frame()
# run method 3 by changing seed
for (i in 1:10) {
  set.seed(i + 500) # change seed in each iteration
  # split the data
  temp_storage <- rep(list(NA), 3)
  select_index <- 1:nrow(image0_blur)
  for (i in 1:3) {
    temp_index <- sample(na.omit(select_index), size = nrow(image0_blur) %/% 3)
    select_index[temp_index] <- NA
    temp_storage[[i]] <- image0_blur[temp_index,]
  }
  data_tr4 <- temp_storage[[1]]
  data_va4 <- temp_storage[[2]]
  data_te4 <- temp_storage[[3]]
  # report accuracy
  a1 <- sum(data_va4$expert == -1)/nrow(data_va4)
  a2 <- sum(data_te4$expert == -1)/nrow(data_te4)
  stability_report4 <- rbind(stability_report4, data.frame(val_accuracy = a1, test_accuracy = a2))
}

# round off the numbers (purely aesthetic purposes)
stability_report4 <- round(stability_report4, digits = 5)
temp_v_mean <- round(mean(stability_report4$val_accuracy), digit = 5)
temp_v_sd <- round(sd(stability_report4$val_accuracy), digit = 5)
temp_t_mean <- round(mean(stability_report4$test_accuracy), digit = 5)
temp_t_sd <- round(sd(stability_report4$test_accuracy),  digit = 5)

stability_report4 <- rbind(stability_report4, 
                           data.frame(val_accuracy = "---------", test_accuracy = "---------"), 
                           data.frame(val_accuracy = temp_v_mean, test_accuracy = temp_t_mean), 
                           data.frame(val_accuracy = temp_v_sd, test_accuracy = temp_t_sd))
rownames(stability_report4) <- c(1:10,"----", "mean", "sd")

stability_report4
```
OBSERVATION: Splitting method4 is most stable, next is method2. 
Method4's val_accuracy is very close to test_accuracy, while other methods have quite different accuracy across validation and testing.
We will be using method4 for the following parts, because only method4 takes advantage of neighborhood dependance.
CONCLUSION: method4 is our best splitting method.
## c) feature importance
potentially:
1. plot histogram of each feature colored by label (just like PART I.c) and maximize the distance between mean
2. clustering using EM & K-means (using 1 feature) 结果和1应该会一样
3. clustering (pairwise) DOABLE?
```{r separation & correlation method}
# obtain the cloudy & clear regions
image_cloud <- image0[which(image0$expert == 1), ]
image_terr <- image0[which(image0$expert == -1), ]
# initialize the vector whose elements will be the average of each features
avg_cloud <- c()
avg_terr <- c()
# put values in
for (i in 1:ncol(image0)){
  avg_cloud[i] <- mean(scale(image_cloud[,i], center = FALSE))
  avg_terr[i] <- mean(scale(image_terr[,i], center = FALSE))
}
# calculate the avg(cloud) - avg(terrain). The feature that maximizes this difference is considered good
avg_diff <- cbind(names(image0), abs(avg_cloud - avg_terr))
colnames(avg_diff) = c('feature', 'mean_difference')
# ignore the expert, x, y row
avg_diff <- data.frame(avg_diff[-3,][-1,][-1,])
# form order
avg_diff$feature <- factor(avg_diff$feature, levels = avg_diff$feature[order(avg_diff$mean_difference, decreasing = TRUE)])
# quant:
avg_diff
# plot
ggplot(data = avg_diff, aes(x = feature, y = mean_difference)) + 
  geom_bar(stat = "identity") +
  labs(title = "standardized difference between class mean by feature")

# correlation
corr <- cbind(names(image0), abs(cor(image0))[3,])
corr <- data.frame(corr[-1,][-1,][-1,])
colnames(corr) <- c('feature', 'corr_btw_label')
rownames(corr) <- 1:8
corr$feature <- factor(corr$feature, levels = corr$feature[order(corr$corr_btw_label, decreasing = TRUE)])
# quant:
corr
# plot:
ggplot(data=corr, aes(x = feature, y = corr_btw_label)) + 
  geom_bar(stat = "identity") +
  labs(title = "correlation between features and labels")
```
we choose the "best features" based on top 3 from the separation & correlation analysis.
## d) generic CV function
```{r}
source("CVgeneric.R")
```
# PART III. Modeling
## a) models & CV
LDA requires no standardization on the feature
```{r splitting method4}
# select training & testing data
tr <- rbind(data_tr4, data_va4)
te <- data_te4

######################################## do not change the below
tr <- tr[tr$expert != 0,]
# tr <- tr[sample(1:nrow(tr), size = 1000),]
tr_ft <- as.data.frame(cbind(tr$NDAI, tr$CORR, tr$SD))
colnames(tr_ft) <- c("NDAI", "CORR", "SD")
tr_lb <- as.factor(tr$expert)
tr <- cbind(tr_ft, tr_lb)
colnames(tr) <- c(colnames(tr_ft), "expert")
te <- te[te$expert != 0,]
te_ft <- as.data.frame(cbind(te$NDAI, te$CORR, te$SD))
colnames(te_ft) <- c("NDAI", "CORR", "SD")
te_lb <- as.factor(te$expert)
te <- cbind(te_ft, te_lb)
colnames(te) <- c(colnames(te_ft), "expert")


LDA_CV <- CVgeneric(lda.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
QDA_CV <- CVgeneric(qda.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
GLM_CV <- CVgeneric(glm.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
tre_CV <- CVgeneric(tree.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)

CV_result <- cbind(LDA_CV, QDA_CV, GLM_CV, tre_CV)
colnames(CV_result) <- c("LDA", "QDA", "GLM", "Tree")
CV_result
```

```{r splitting method2}
# select training & testing data
tr <- rbind(data_tr2, data_va2)
te <- data_te2

######################################## do not change the below
tr <- tr[tr$expert != 0,]
# tr <- tr[sample(1:nrow(tr), size = 1000),]
tr_ft <- as.data.frame(cbind(tr$NDAI, tr$CORR, tr$SD))
colnames(tr_ft) <- c("NDAI", "CORR", "SD")
tr_lb <- as.factor(tr$expert)
tr <- cbind(tr_ft, tr_lb)
colnames(tr) <- c(colnames(tr_ft), "expert")
te <- te[te$expert != 0,]
te_ft <- as.data.frame(cbind(te$NDAI, te$CORR, te$SD))
colnames(te_ft) <- c("NDAI", "CORR", "SD")
te_lb <- as.factor(te$expert)
te <- cbind(te_ft, te_lb)
colnames(te) <- c(colnames(te_ft), "expert")

LDA_CV <- CVgeneric(lda.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
QDA_CV <- CVgeneric(qda.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
GLM_CV <- CVgeneric(glm.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)
tre_CV <- CVgeneric(tree.changed, tr_feature = tr_ft, tr_label = tr_lb, K = 10, loss_fn = zero_one_loss)

CV_result <- cbind(LDA_CV, QDA_CV, GLM_CV, tre_CV)
colnames(CV_result) <- c("LDA", "QDA", "GLM", "Tree")
CV_result
```
OBSERVATION: From CV, QDA is the best classifier.
OBSERVATION: splitting method3 has higher CV scores than splitting method2.
## b) ROC
```{r select the best splitting method}
# select training & testing data
tr <- rbind(data_tr4, data_va4)
te <- data_te4

# this time we will preserve all features (easier for further analysis)

# treat logistic regression differently, because this one is a little bit trickier than other:
tr_glm <- as.data.frame(cbind(tr$NDAI, tr$CORR, tr$SD, (tr$expert + 1)/2))
colnames(tr_glm) <- c("NDAI", "CORR", "SD", "expert")
```

```{r generating ROC}
# LDA
LDA_fit <- lda(expert ~ NDAI + CORR + SD, data = tr)
LDA_pred <- predict(LDA_fit, te)
    # choose the posterior probability column carefully, it may be 
    # lda.pred$posterior[,1] or lda.pred$posterior[,2], depending on your factor levels 
pred_LDA <- prediction(LDA_pred$posterior[,2], labels = te$expert)
perf_LDA <- performance(pred_LDA, "tpr", "fpr")
auc_LDA <- performance(pred_LDA, measure = "auc")

# QDA
QDA_fit <- qda(expert ~ NDAI + CORR + SD, data = tr)
QDA_pred <- predict(QDA_fit, te)
pred_QDA <- prediction(QDA_pred$posterior[,2], labels = te$expert)
perf_QDA <- performance(pred_QDA, "tpr", "fpr")
auc_QDA <- performance(pred_QDA, measure = "auc")

# GLM
GLM_fit <- glm(expert ~., data = tr_glm, family = binomial)
GLM_prob <- predict(GLM_fit, te, type = "response")
pred_GLM <- prediction(GLM_prob, labels = te$expert)
perf_GLM <- performance(pred_GLM, "tpr", "fpr")
auc_GLM <- performance(pred_GLM, measure = "auc")

# Decision Tree
DT_fit <- rpart::rpart(expert ~ NDAI + CORR + SD, data = tr, method = "class")
DT_prob <- predict(DT_fit, te)
pred_DT <- prediction(DT_prob[,2], labels = te$expert)
perf_DT <- performance(pred_DT, "tpr", "fpr")
auc_DT <- performance(pred_DT, measure = "auc")
```

```{r AUC analysis}
AUC <- rbind(auc_LDA@y.values[[1]], auc_QDA@y.values[[1]], auc_GLM@y.values[[1]], auc_DT@y.values[[1]])
rownames(AUC) <- c("LDA", "QDA", "GLM", "Decision Tree")
colnames(AUC) <- "AUC"
AUC
```

```{r cutoff}
# this is the cutoff index
# find cut_off that is closest to the (0,1) point
closest_to <- function(x, y, target_x, target_y){
  dist <- sqrt((x-target_x)^2 + (y-target_y)^2)
  temp_index <- which.min(dist)
  return(c(x=x[temp_index], y=y[temp_index]))
}

LDA_cut <- closest_to(x = perf_LDA@x.values[[1]], y = perf_LDA@y.values[[1]], target_x = 0, target_y = 1)
QDA_cut <- closest_to(x = perf_QDA@x.values[[1]], y = perf_QDA@y.values[[1]], target_x = 0, target_y = 1)
GLM_cut <- closest_to(x = perf_GLM@x.values[[1]], y = perf_GLM@y.values[[1]], target_x = 0, target_y = 1)
DT_cut <- closest_to(x = perf_DT@x.values[[1]], y = perf_DT@y.values[[1]], target_x = 0, target_y = 1)

temp <- rbind(LDA_cut, QDA_cut, GLM_cut, DT_cut)
cutoff <- c(perf_LDA@alpha.values[[1]][min(which(perf_LDA@x.values[[1]] == LDA_cut[1]))],
            perf_QDA@alpha.values[[1]][min(which(perf_QDA@x.values[[1]] == QDA_cut[1]))],
            perf_GLM@alpha.values[[1]][min(which(perf_GLM@x.values[[1]] == GLM_cut[1]))],
            perf_DT@alpha.values[[1]][min(which(perf_DT@x.values[[1]] == DT_cut[1]))])
cutoff <- cbind(temp, cutoff)
rownames(cutoff) <- c("LDA", "QDA", "Log_reg", "Dec_tree")
colnames(cutoff) <- c("fpr", "tpr", "cutoff")
cutoff
```

```{r graphing ROC}
# this step is slow
range <- c(0,1)
par(mfrow=c(2,2))
plot(perf_LDA, colorize = TRUE, main = "LDA ROC", xlim = range, ylim = range)
  abline(a = 0, b = 1)
  points(x = LDA_cut[1], y = LDA_cut[2])
  text(0.8, 0.2, paste("AUC = ", round(AUC[1], digits = 4)))
  text(0.2, 0.8, paste("cutoff = ", round(cutoff[1,3], digits = 4)))
plot(perf_QDA, colorize = TRUE, main = "QDA ROC", xlim = range, ylim = range)
  abline(a = 0, b = 1)
  points(x = QDA_cut[1], y = QDA_cut[2])
  text(0.8, 0.2, paste("AUC = ", round(AUC[2], digits = 4)))
  text(0.2, 0.8, paste("cutoff = ", round(cutoff[2,3], digits = 4)))
plot(perf_GLM, colorize = TRUE, main = "Logistic Regression ROC", xlim = range, ylim = range)
  abline(a = 0, b = 1)
  points(x = GLM_cut[1], y = GLM_cut[2])
  text(0.8, 0.2, paste("AUC = ", round(AUC[3], digits = 4)))
  text(0.2, 0.8, paste("cutoff = ", round(cutoff[3,3], digits = 4)))
plot(perf_DT, colorize = TRUE, main = "Decision Tree ROC", xlim = range, ylim = range)
  abline(a = 0, b = 1)
  points(x = DT_cut[1], y = DT_cut[2])
  text(0.8, 0.2, paste("AUC = ", round(AUC[4], digits = 4)))
  text(0.2, 0.8, paste("cutoff = ", round(cutoff[4,3], digits = 4)))
```
OBSERVATION: decision tree has horrible ROC result, we are not going to consider decision tree as our best model.
OBSERVATION: QDA has highest AUC. LDA and logistic regression have similar AUC.
OBSERVATION: because of the above observations, the data is not linearly separable in high dimensions.
## c) IC test / likelihood
```{r AIC+BIC}
# LDA
LDA_pred <- predict(LDA_fit, tr)
LDA_fit_cloud <- tr[which(LDA_pred$class == 1),]
LDA_fit_clear <- tr[which(LDA_pred$class == -1),]
LDA_prior <- LDA_fit$prior
LDA_cov <- cov(LDA_fit_cloud[,4:6])*LDA_prior[2] + cov(LDA_fit_clear[,4:6])*LDA_prior[1]
LDA_cloud_mean <- colMeans(LDA_fit_cloud[,4:6])
LDA_clear_mean <- colMeans(LDA_fit_clear[,4:6])
log_lik <- 0
for (r in 1:nrow(tr)){
  log_lik <- log_lik + log(LDA_prior[2]*dmvnorm(x=as.vector(tr[,4:6][r,]), mean=LDA_cloud_mean, sigma=LDA_cov) + LDA_prior[1]*dmvnorm(x=as.vector(tr[,4:6][r,]), mean=LDA_clear_mean, sigma=LDA_cov))
}
log_lik_LDA <- as.numeric(log_lik)
AIC_LDA  <- -2*log_lik_LDA + 5

# QDA
QDA_pred <- predict(QDA_fit, tr)
QDA_fit_cloud <- tr[which(QDA_pred$class == 1),]
QDA_fit_clear <- tr[which(QDA_pred$class == -1),]
QDA_prior <- QDA_fit$prior
QDA_cloud_mean <- colMeans(QDA_fit_cloud[,4:6])
QDA_clear_mean <- colMeans(QDA_fit_clear[,4:6])
log_lik <- 0
for (r in 1:nrow(tr)){
  log_lik <- log_lik + log(LDA_prior[2]*dmvnorm(x=as.vector(tr[,4:6][r,]), mean=LDA_cloud_mean, sigma=cov(LDA_fit_cloud[,4:6])) + LDA_prior[1]*dmvnorm(x=as.vector(tr[,4:6][r,]), mean=LDA_clear_mean, sigma=cov(LDA_fit_clear[,4:6])))
}
log_lik_QDA <- as.numeric(log_lik)
AIC_QDA  <- -2*log_lik_QDA + 6

AIC_result <- round(c(AIC_LDA, AIC_QDA, AIC(GLM_fit)), digits = 0)
BIC_result <- round(c(-2*log_lik_LDA + log(nrow(tr))*5, -2*log_lik_QDA + log(nrow(tr))*6, BIC(GLM_fit)), digit = 0)
IC_test <- rbind(AIC_result, BIC_result)
colnames(IC_test) <- c("LDA", "QDA", "log_reg")

IC_test
```

# PART IV. Diagnostics
splitting method: smoothing + simple random sampling
fitting model: QDA
model testing error = 9.3332%
```{r reset}
# below was masked during AIC test, now restore:
QDA_fit <- qda(expert ~ NDAI + CORR + SD, data = tr)
QDA_pred <- predict(QDA_fit, te)
```

## a) in-depth analysis 
```{r convergence test - model error}
set.seed(666)
conv_result <- conv_test(input_data = image0_blur, model = qda, K = 100)
ggplot(data = conv_result[[1]], aes(x = train_size, y = error)) +
  geom_line() + 
  xlim(0,1) + ylim(0,1) + coord_fixed(ratio = 1) + 
  labs(title = "Convergence Test of QDA model")
```
OBSERVATION: QDA model's testing error converges extremeley fast.
```{r convergence test - feature mean}
set.seed(123)
para_result <- conv_result[[2]]
para_cloud <- para_result[para_result$label == 1,]
para_clear <- para_result[para_result$label == -1,]
i1 <- ggplot(data = para_cloud, aes(x = train_size)) +
  geom_line(aes(y = NDAI), colour = "red") + geom_line(aes(y = CORR), colour = "blue") + geom_line(aes(y = SD), colour = "green") + labs(title = "Concergence of QDA (cloudy)") + ylab(label = "fitted mean") + xlim(0,1) + ylim(-1,11)
i2 <- ggplot(data = para_clear, aes(x = train_size)) +
  geom_line(aes(y = NDAI), colour = "red") + geom_line(aes(y = CORR), colour = "blue") + geom_line(aes(y = SD), colour = "green") + labs(title = "Concergence of QDA (clear)") + ylab(label = "fitted mean") + xlim(0,1) + ylim(-1,11)
grid.arrange(i1,i2, nrow=1)
```
OBSERVATION: feature mean of fitted model converges also very fast.
```{r parameter test - cutoff}
# tuning parameters
input_data <- image0_blur
K <- 100

set.seed(123666)
fold_index <- createFolds(c(1:nrow(input_data)), k=K, list = TRUE)
conv_test <- input_data[fold_index[[K]],]
pred_err <- numeric()
means <- data.frame()
cuts <- numeric()
for (i in 1:K) {
  conv_train <- data.frame()
  which_fold <- sample(1:K, i, replace = FALSE)
  for (j in which_fold) {
    conv_train <- rbind(conv_train, input_data[fold_index[[j]],])
  }
  temp_model <- qda(data = conv_train, expert ~ NDAI + CORR + SD)
  temp_pred <- predict(temp_model, conv_test)
  temp_pred <- prediction(temp_pred$posterior[,2], labels = conv_test$expert)
  temp_perf <- performance(temp_pred, "tpr", "fpr")
  QDA_cut <- closest_to(x = temp_perf@x.values[[1]], y = temp_perf@y.values[[1]], target_x = 0, target_y = 1)
  temp_cutoff <- temp_perf@alpha.values[[1]][min(which(temp_perf@x.values[[1]] == QDA_cut[1]))]
  cuts <- c(cuts, temp_cutoff)
}
para_result <- data.frame(train_size = round((1:K)/K, digits = 3), cutoff = cuts)

ggplot(data = para_result, aes(x = train_size, y = cutoff)) +
  geom_line(colour = "blue") + labs(title = "Parameter Estimation stability & convergence") + ylab(label = "ROC cutoff") + xlim(0,1) + ylim(0,1) + coord_fixed(ratio = 1)
```
OBSERVATION: parameter (ROC cutoff) also converges very fast.
## b) misclassification trend
```{r class percentage in misclassification}
tr <- rbind(data_tr4, data_va4)
te <- data_te4
QDA_fit <- qda(expert ~ NDAI + CORR + SD, data = tr)
QDA_pred <- predict(QDA_fit, te)
misclassified <- te[QDA_pred$class != te$expert,]

class_percentage <- rbind(percent(table(num2class(misclassified$expert))/nrow(misclassified)),
                          percent(table(num2class(image0_labeled$expert))/nrow(image0_labeled)))
rownames(class_percentage) <- c("misclassified", "total_labeled")
colnames(class_percentage) <- c("clear", "cloudy")
class_percentage
```
OBSERVATION: class percentage in misclassification is close to total class percentage
CONCLUSION: the model is not biased into producing specific kind of misclassification (i.e. most of the misclassifications are false clear)
```{r summary about the model}
# let's look at the specific part of image3 (where most misclassification of "cloudy" label come from)
# manually selelect the interesting part
image3_interest <- image3[image3$expert == 1 & image3$x <= 270 & image3$y >= 190,]
image3_interest <- image3_interest[-which(image3_interest$x >= 250 & image3_interest$y >= 300),]
image3_interest <- image3_interest[-which(image3_interest$x >= 125 & image3_interest$y >= 350),]
image3_interest <- image3_interest[-which(image3_interest$x <= 175 & image3_interest$y <= 225),]
image3_interest <- image3_interest[-which(image3_interest$x <= 100 & image3_interest$y <= 275),]

i1 <- ggplot(data = misclassified, aes(x = x, y = y)) +
  geom_point(colour = num2col(misclassified$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  stat_ellipse(data = image3_interest, aes(x = x, y = y), colour = "red") + 
  labs(title = "misclassified data") + scale_y_reverse()
i2 <- ggplot(data = image3, aes(x = x, y = y)) +
  geom_point(colour = num2col(image3$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  stat_ellipse(data = image3_interest, aes(x = x, y = y), colour = "red") + 
  labs(title = "Image 3") + scale_y_reverse()
grid.arrange(i1,i2, nrow=1)
```
OBSERVATION: this graph looks like data image3.
OBSERVATION: even though the testing data pixels are randomly selected, the misclassified cloudy labels (the white dots in the red oval) seem to be mainly from the image3. There definitely exist some trend in the misclassified data.
CONCLUSION: most of misclassified cloudy data comes from image3. This is perhaps because image3 is taken at different place / different time, and image3's cloud is somehow different from image1 and 2. But most data comes from image1 and 2, thus the model trained based on image1 and 2 may not apply well to cloudy areas in image3. For the clear parts that are misclassified, it's harder to find some evidence from this graph alone.
```{r histogram of misclassified}
# use this histogram to compare with the histogram of all data
hist_ft <- feature_hist_by_class(misclassified, spec = "of misclassified")
hist_ft[[1]]
hist_ft[[2]]
hist_ft[[3]]
grid.arrange(hist_ft[[4]], hist_ft[[5]], hist_ft[[6]], hist_ft[[7]], hist_ft[[8]])
```
OBSERVATION: misclassified data have similar shape of distribution of feature as the non-misclassified data.
OBSERVATION: compare the NDAI with the total data NDAI by class: cloudy areas on average have higher NDAI than clear areas on all data. (that is, the blue should be on the right side). However, this graph shows the opposite. 
OBSERVATION: same with SD, cloudy (blue) should have longer tails than clear (red).
OBSERVATION: CORR behaves differently from SD and NDAI. The misclassified data have very similar distribution in CORR for both classes.
OBSERVATION: 5 radiance readings show that the misclassified true clear data tend to be the ones with lower radiance readings. Before, we dicussesed that the radiance readings seem to be bi-modal. From the graph above, we can say that these misclassified data (whose true label is clear) seem to come from the left mode (one that is also lower). It seems that the trend is those misclassified data seem to come from specific radiance reading range.
```{r numeric analysis on misclassification}
# standardized difference in feature 
# misclassified V.S. total data 
# (this one is not that important)
feat_mean_diff_std(img1 = misclassified, imgTrue = image0_labeled)

# misclassified V.S. total data (cloudy only)
image0_cloud <- image0[image0$expert == 1,]
misclassified_cloud <- misclassified[misclassified$expert == 1,]
cloud_diff <- feat_mean_diff_std(img1 = misclassified_cloud, imgTrue = image0_cloud)

# misclassified V.S. total data (clear only)
image0_clear <- image0[image0$expert == -1,]
misclassified_clear <- misclassified[misclassified$expert == -1,]
clear_diff <- feat_mean_diff_std(img1 = misclassified_clear, imgTrue = image0_clear)

difference <- as.data.frame(cbind(cloud_diff[,4], clear_diff[,4]))
colnames(difference) <- c("false clear", "false cloudy")
rownames(difference) <- rownames(cloud_diff)
difference
```
REFERENCE: if the absolute value of standardized mean difference is large, then it means that for this feature, the misclassified data is deviated from the whole data.
REFERENCE: we should mainly focus on NDAI, SD, CORR, because they are the only features used in the model
OBSERVATION & CONCLUSION: false cloudy has very different NDAI and SD distribution than the total data. (the standardized mean difference is above / close to 2)
OBSERVATION: in comparison, false clear are a little bit hard to tell by numbers (such mean difference can be well explained by due-to-chance). Keep in mind that the figure shows most false-clear are from image3. We need to do more analysis on the false-clear misclassification.
```{r close-up examination of false-clear}
image3_other_cloud <- image3[-as.numeric(rownames(image3_interest)),]
image3_other_cloud <- image3_other_cloud[image3_other_cloud$expert == 1,]
image0_other_cloud <- rbind(image1_labeled[image1_labeled$expert == 1,], 
                            image2_labeled[image2_labeled$expert == 1,],
                            image3_other_cloud)
image0_cloud <- rbind(cbind(image3_interest, origin = "image3_big_cloud"), 
                      cbind(misclassified[misclassified$expert == 1,], origin = "false_clear"),
                      cbind(image0_other_cloud, origin = "other"))
hist_ft <- feature_hist_by_class_mod(input_image = image0_cloud)
hist_ft[[1]]
hist_ft[[2]]
hist_ft[[3]]
```
OBSERVATION: strangely, the big cloud in the image3 has distribution in all 8 features the same as the rest of clouds.
OBSERVATION: however, the big cloud in image3 and false clear have more similar distribution than false clear compared with all clouds.
## c) new model
NEW MODEL: take square root of all 3 features, still use QDA + blurring method
JUSTIFICATION: to decrease feature variance
```{r CV for new model}
tr <- rbind(data_tr4, data_va4)
te <- rbind(data_te4)

# square root the 3 features: NDAI, SD, CORR
tr$NDAI <- log(tr$NDAI - min(tr$NDAI) + 1)
tr$SD <- log(tr$SD - min(tr$SD) + 1)
tr$CORR <- log(tr$CORR - min(tr$CORR) + 1)
te$NDAI <- log(te$NDAI - min(te$NDAI) + 1)
te$SD <- log(te$SD - min(te$SD) + 1)
te$CORR <- log(te$CORR - min(te$CORR) + 1)

# 10-fold CV:
set.seed(789234)
QDA_CV_changed <- CVgeneric(qda.changed, tr_feature = tr[,4:6], tr_label = tr[,3], K = 10, loss_fn = zero_one_loss)

# compare with old method
QDA_compare <- cbind(QDA_CV, QDA_CV_changed)
colnames(QDA_compare) <- c("old_model", "new_model")
QDA_compare
```
OBSERVATION: Cross Validation improved by 1 percent.
```{r}
QDA_fit <- qda(expert ~ NDAI + CORR + SD, data = tr)
QDA_pred <- predict(QDA_fit, te)
misclassified_new <- te[QDA_pred$class != te$expert,]

class_percentage <- rbind(percent(table(num2class(misclassified_new$expert))/nrow(misclassified_new)),
                          percent(table(num2class(image0_labeled$expert))/nrow(image0_labeled)))
rownames(class_percentage) <- c("misclassified", "total_labeled")
colnames(class_percentage) <- c("clear", "cloudy")
class_percentage
```
OBSERVATION: misclassification is not biased either. same as original model
```{r misclassification - new model}
i1 <- ggplot(data = misclassified_new, aes(x = x, y = y)) +
  geom_point(colour = num2col(misclassified_new$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  stat_ellipse(data = image3_interest, aes(x = x, y = y), colour = "red") + 
  labs(title = "misclassified data (new model)") + scale_y_reverse()
i2 <- ggplot(data = misclassified, aes(x = x, y = y)) +
  geom_point(colour = num2col(misclassified$expert), size = 0.9) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  stat_ellipse(data = image3_interest, aes(x = x, y = y), colour = "red") + 
  labs(title = "misclassified data (old model)") + scale_y_reverse()
grid.arrange(i1,i2, nrow=1)
```
OBSERVATION: The "big cloud" in image 3 is still causing problem, as one can see in the red oval. However, the new model has more misclassified clear data that is not part of systematic error (total number of misclassification is roughly the same, but there are more misclassified clear outside the chunk), indicating that the new model improves systematic error (just slightly) on the misclassified clear (false cloudy).
```{r histogram of misclassified - new model}
# use this histogram to compare with the histogram of all data
hist_ft <- feature_hist_by_class(misclassified_new, spec = "of misclassified (new model)")
hist_ft[[1]]
hist_ft[[2]]
hist_ft[[3]]
grid.arrange(hist_ft[[4]], hist_ft[[5]], hist_ft[[6]], hist_ft[[7]], hist_ft[[8]])
```

## d) using a different splitting
splitting method: forge + simple random sampling
fitting model: QDA
model testing error = 13.5264%
```{r reset data splitting}
tr <- rbind(data_tr3, data_va3)
te <- data_te3
```
### a) in-depth analysis 
```{r convergence test - model error2}
set.seed(1973)
K <- 3

temp_divided <- c(divide_image(image1_labeled, K), divide_image(image2_labeled, K), divide_image(image3_labeled, K))
K <- length(temp_divided)
conv_test <- temp_divided[[K]]
pred_err <- numeric()
for (i in 1:(K-1)) {
  conv_train <- data.frame()
  temp_index <- sample(1:(K-1), i, replace = FALSE)
  for (j in temp_index) {
    conv_train <-rbind(conv_train, temp_divided[[j]])
  }
  temp_model <- qda(data = conv_train, expert ~ NDAI + CORR + SD)
  temp_pred <- predict(temp_model, conv_test)$class
  pred_err <- c(pred_err, 1 - zero_one_loss(conv_test$expert, temp_pred))
}
convergence_result <- data.frame(train_size = round((1:(K-1))/K, digits = 3), error = pred_err)
convergence_result

ggplot(data = convergence_result, aes(x = train_size, y = error)) +
  geom_line() + 
  xlim(0,1) + ylim(0,1) + coord_fixed(ratio = 1) + 
  labs(title = "Convergence Test of QDA model")
```
Convergence takes longer than smoothing method splitting. Given that method3 already has almost ten times observations, it still takes longer to converge. Although this one is smoother (it fluctuates less after convergence). This is most likely due to larger number of observations
### b) misclassification trend
```{r class percentage in misclassification2}
QDA_fit <- qda(expert ~ NDAI + CORR + SD, data = tr)
QDA_pred <- predict(QDA_fit, te)
misclassified <- cbind(te[QDA_pred$class != te$expert,])

class_percentage <- rbind(percent(table(num2class(misclassified$expert))/nrow(misclassified)),
                          percent(table(num2class(image0_labeled$expert))/nrow(image0_labeled)))
rownames(class_percentage) <- c("misclassified", "total_labeled")
colnames(class_percentage) <- c("clear", "cloudy")
class_percentage

nrow(misclassified) / nrow(data_te2)
```
OBSERVATION: class percentage in misclassification is different than total class percentage
OBSERVATION: testing error went up from 9.3332% to 14.6620% (after switching data splitting method from smoothing to method3)
CONCLUSION: after changing splitting method, the model is biased into producing more false clear error. also the model produces more testing errors.
```{r summary about the model2}
# let's look at the specific part of image3 (where most misclassification of "cloudy" label come from)
# manually selelect the interesting part
i1 <- ggplot(data = misclassified, aes(x = x, y = y)) +
  geom_point(colour = num2col(misclassified$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "misclassified data (splitting method 3)") + scale_y_reverse()
i2 <- ggplot(data = data_te2, aes(x = x, y = y)) +
  geom_point(colour = num2col(data_te2$expert), size = 0.1) +
  xlim(50,400) + ylim(0,400) + coord_fixed(ratio = 1) +
  labs(title = "testing data (splitting method 3)") + scale_y_reverse()
grid.arrange(i1, i2, nrow=1)
```
OBSERVATION: most of misclassified data come from certain areas of the testing data
```{r histogram of misclassified2}
# use this histogram to compare with the histogram of all data
hist_ft <- feature_hist_by_class(misclassified, spec = "of misclassified")
hist_ft[[1]]
hist_ft[[2]]
hist_ft[[3]]
grid.arrange(hist_ft[[4]], hist_ft[[5]], hist_ft[[6]], hist_ft[[7]], hist_ft[[8]])
```
OBSERVATION: 
```{r numeric analysis on misclassification2}
# standardized difference in feature 
# misclassified V.S. total data 
# (this one is not that important)
feat_mean_diff_std(img1 = misclassified, imgTrue = image0_labeled)

# misclassified V.S. total data (cloudy only)
image0_cloud <- image0[image0$expert == 1,]
misclassified_cloud <- misclassified[misclassified$expert == 1,]
cloud_diff <- feat_mean_diff_std(img1 = misclassified_cloud, imgTrue = image0_cloud)

# misclassified V.S. total data (clear only)
image0_clear <- image0[image0$expert == -1,]
misclassified_clear <- misclassified[misclassified$expert == -1,]
clear_diff <- feat_mean_diff_std(img1 = misclassified_clear, imgTrue = image0_clear)

difference <- as.data.frame(cbind(cloud_diff[,4], clear_diff[,4]))
colnames(difference) <- c("false clear", "false cloudy")
rownames(difference) <- rownames(cloud_diff)
difference
```
OBSERVATION: this part is similar to part b).
CONCLUSION: after changing the data splitting method, result in b) changes: the big cloud in image3 is not introducing systematic error anymore. However, 